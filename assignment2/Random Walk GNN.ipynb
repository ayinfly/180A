{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f016489f",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: \n",
    "\n",
    "In this assignment, implement the Node2Vec algorithm, a random-walk-based GNN, to learn node embeddings. Train a classifier using the learned embeddings to predict node labels.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Cora dataset: The dataset consists of 2,708 nodes (scientific publications) with 5,429 edges (citations between publications). Each node has a feature vector of size 1,433, and there are 7 classes (research topics).\n",
    "Skeleton Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c492a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e5a5afe61e408a9fc0361e3fceb5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:13<00:00,  7.29it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:13<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9608745574951172\n",
      "Epoch 10, Loss: 1.2374218702316284\n",
      "Epoch 20, Loss: 0.8905270099639893\n",
      "Epoch 30, Loss: 0.7458568811416626\n",
      "Epoch 40, Loss: 0.6765410304069519\n",
      "Epoch 50, Loss: 0.6362488865852356\n",
      "Epoch 60, Loss: 0.6089761257171631\n",
      "Epoch 70, Loss: 0.5889682769775391\n",
      "Epoch 80, Loss: 0.5732861161231995\n",
      "Epoch 90, Loss: 0.5605018734931946\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec  # Importing Node2Vec for the random walk\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "\n",
    "def train():\n",
    "    for epoch in range(100):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get node embeddings as input\n",
    "        output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "        \n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ee022",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Node2Vec generates node embeddings by simulating random walks on the graph. These walks capture structural properties of nodes.\n",
    "The generated embeddings are then used to train a classifier for predicting node labels.\n",
    "The Cora dataset is a benchmark graph where nodes are papers and edges are citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b3004",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "\n",
    "4. What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "5. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "6. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "8. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "\n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "\n",
    "## No points, just for you to think about\n",
    "7. What would happen if we removed self-loops from the graph before training Node2Vec?\n",
    "\n",
    "9. What would happen if we applied normalization to the node embeddings before feeding them to the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c2244e",
   "metadata": {},
   "source": [
    "### 1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b9b3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcac478fec404398b93970a9b3c02b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 250/250 [00:45<00:00,  5.44it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 250/250 [00:46<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0586254596710205\n",
      "Epoch 10, Loss: 1.2902240753173828\n",
      "Epoch 20, Loss: 0.9253613948822021\n",
      "Epoch 30, Loss: 0.7644137740135193\n",
      "Epoch 40, Loss: 0.6871922612190247\n",
      "Epoch 50, Loss: 0.6434270739555359\n",
      "Epoch 60, Loss: 0.6147224307060242\n",
      "Epoch 70, Loss: 0.5941963791847229\n",
      "Epoch 80, Loss: 0.5785537958145142\n",
      "Epoch 90, Loss: 0.56610506772995\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=500, workers=2)\n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910efae3",
   "metadata": {},
   "source": [
    "Increasing the number of walks in Node2Vec provides more training data by exploring more paths in the graph, leading to more robust and detailed node embeddings. This should result in faster convergence and lower training loss, as the model captures the graph's structure more effectively. We can see that this does in fact lead to a lower training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f071852",
   "metadata": {},
   "source": [
    "### 2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd2b35eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8107d459bcc54f13baa0bcf37e02376b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:04<00:00, 23.68it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:04<00:00, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.02813458442688\n",
      "Epoch 10, Loss: 1.2759928703308105\n",
      "Epoch 20, Loss: 0.9090166091918945\n",
      "Epoch 30, Loss: 0.7447628974914551\n",
      "Epoch 40, Loss: 0.6635621190071106\n",
      "Epoch 50, Loss: 0.6174379587173462\n",
      "Epoch 60, Loss: 0.5875948071479797\n",
      "Epoch 70, Loss: 0.5660608410835266\n",
      "Epoch 80, Loss: 0.549502968788147\n",
      "Epoch 90, Loss: 0.5361846089363098\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=10, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "\n",
    "def train():\n",
    "    for epoch in range(100):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get node embeddings as input\n",
    "        output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "        \n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893c039",
   "metadata": {},
   "source": [
    "Reducing the walk length in Node2Vec limits the exploration range of each random walk, causing the embeddings to focus more on the local neighborhood of each node. This would lead to less comprehensive structural information, making the embeddings more biased toward capturing close connections rather than the broader graph structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f470470e",
   "metadata": {},
   "source": [
    "### 4. What would happen if we used directed edges instead of undirected edges for the random walks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a648041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcb8b5293f24c5082a20400464cf9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:14<00:00,  7.09it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:14<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9939035177230835\n",
      "Epoch 10, Loss: 1.294539213180542\n",
      "Epoch 20, Loss: 0.9360954761505127\n",
      "Epoch 30, Loss: 0.7753670811653137\n",
      "Epoch 40, Loss: 0.6957798004150391\n",
      "Epoch 50, Loss: 0.6492462158203125\n",
      "Epoch 60, Loss: 0.6185664534568787\n",
      "Epoch 70, Loss: 0.5967667102813721\n",
      "Epoch 80, Loss: 0.580217719078064\n",
      "Epoch 90, Loss: 0.566978394985199\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=False)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "\n",
    "def train():\n",
    "    for epoch in range(100):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get node embeddings as input\n",
    "        output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "        \n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b53ed2",
   "metadata": {},
   "source": [
    "Using directed edges in Node2Vec means that the random walks respect the direction of the edges, capturing the asymmetric relationships between nodes. This should lead to embeddings that reflect the influence of directionality in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295412b1",
   "metadata": {},
   "source": [
    "### 5. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745fb13",
   "metadata": {},
   "source": [
    "Adding more features to the nodes provides the model with a deeper understanding of each node, potentially capturing more complex patterns and relationships in the data. However, this also increases the risk of overfitting, especially if the additional features are not informative or if the dataset is small. If the data isn't as complex as the model it would lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e02831",
   "metadata": {},
   "source": [
    "### 6. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11fdd04",
   "metadata": {},
   "source": [
    "Using a dataset with more classes increases the complexity of the classification task, requiring the model to learn more nuanced differences between the classes. This often leads to lower initial performance, as the model needs to capture a broader range of features, but it can improve with sufficient data and training to handle the increased diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81159238",
   "metadata": {},
   "source": [
    "### 8. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d07e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08105dd3454407f8cb78add48c2dfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:13<00:00,  7.22it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:14<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9860632419586182\n",
      "Epoch 10, Loss: 1.1065382957458496\n",
      "Epoch 20, Loss: 0.7770000696182251\n",
      "Epoch 30, Loss: 0.659434974193573\n",
      "Epoch 40, Loss: 0.5973513126373291\n",
      "Epoch 50, Loss: 0.5561709403991699\n",
      "Epoch 60, Loss: 0.5259265899658203\n",
      "Epoch 70, Loss: 0.502881646156311\n",
      "Epoch 80, Loss: 0.4844331443309784\n",
      "Epoch 90, Loss: 0.46912264823913574\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=128, walk_length=30, num_walks=200, workers=2)\n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(128, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "\n",
    "def train():\n",
    "    for epoch in range(100):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get node embeddings as input\n",
    "        output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "        \n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70054a5",
   "metadata": {},
   "source": [
    "Increasing the embedding dimension to 128 allows the model to capture more detailed and complex patterns in the graph, leading to improved performance as it learns richer representations. However, this also increases the training time and computational cost due to the larger size of the embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
