{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d48fe682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9547138214111328\n",
      "Epoch 10, Loss: 0.6605719327926636\n",
      "Epoch 20, Loss: 0.1383318454027176\n",
      "Epoch 30, Loss: 0.030570585280656815\n",
      "Epoch 40, Loss: 0.01079611573368311\n",
      "Epoch 50, Loss: 0.005712065380066633\n",
      "Epoch 60, Loss: 0.003950811456888914\n",
      "Epoch 70, Loss: 0.0031179855577647686\n",
      "Epoch 80, Loss: 0.002624556655064225\n",
      "Epoch 90, Loss: 0.002282576635479927\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18442",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "GCN aggregates features from a node’s neighbors using graph convolutions. This allows the network to learn representations based on both node features and graph structure.\n",
    "The Cora dataset is used to classify nodes into one of 7 research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb882b",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "\n",
    "1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "\n",
    "4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "\n",
    "Extra credit: \n",
    "1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?\n",
    "\n",
    "## No points, just for you to think about:\n",
    "1. What would happen if we applied dropout to the node features during training? How would it affect the model’s generalization?\n",
    "2. What would happen if we used mean-pooling instead of summing the messages in the GCN layers?\n",
    "3. What would happen if we pre-trained the node features using a different algorithm, like Node2Vec, before feeding them into the GCN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19957f",
   "metadata": {},
   "source": [
    "### 1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157defbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9440739154815674\n",
      "Epoch 10, Loss: 0.7493696212768555\n",
      "Epoch 20, Loss: 0.1318102329969406\n",
      "Epoch 30, Loss: 0.016000602394342422\n",
      "Epoch 40, Loss: 0.003237819531932473\n",
      "Epoch 50, Loss: 0.0012195336166769266\n",
      "Epoch 60, Loss: 0.0007049707346595824\n",
      "Epoch 70, Loss: 0.0005119502893649042\n",
      "Epoch 80, Loss: 0.00041591981425881386\n",
      "Epoch 90, Loss: 0.0003577024326659739\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92563785",
   "metadata": {},
   "source": [
    "Using more layers would most likely make the model more accurate using loss metrics, but it would also cause over smoothing as the nodes will be grouped together even more, causing a loss of information from an extra layer of grouping. We can see that the loss does decrease significantly from 0.002 to 0.0003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212edb4a",
   "metadata": {},
   "source": [
    "### 2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf979e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9572679996490479\n",
      "Epoch 10, Loss: 0.08742734789848328\n",
      "Epoch 20, Loss: 0.003399561857804656\n",
      "Epoch 30, Loss: 0.0006251619779504836\n",
      "Epoch 40, Loss: 0.00026571392663754523\n",
      "Epoch 50, Loss: 0.00018113326223101467\n",
      "Epoch 60, Loss: 0.00015136782894842327\n",
      "Epoch 70, Loss: 0.00013669671898242086\n",
      "Epoch 80, Loss: 0.00012743067054543644\n",
      "Epoch 90, Loss: 0.00012043907190673053\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdb3a5",
   "metadata": {},
   "source": [
    "Using a larger hidden dimension would allow the model to learn more features without having to worry about over smoothening as the model will group cells together the same amount of times, but instead just have more dimensionality allowing it to learn more connections. This may lead to overfitting if the model is too complex. We can see an even larger decrease in loss from 0.002 to 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5594e",
   "metadata": {},
   "source": [
    "### 3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8ff54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.1052041053771973\n",
      "Epoch 10, Loss: 1.4802534580230713\n",
      "Epoch 20, Loss: 1.0245342254638672\n",
      "Epoch 30, Loss: 0.6691532135009766\n",
      "Epoch 40, Loss: 0.42247724533081055\n",
      "Epoch 50, Loss: 0.2708204686641693\n",
      "Epoch 60, Loss: 0.18145246803760529\n",
      "Epoch 70, Loss: 0.12838099896907806\n",
      "Epoch 80, Loss: 0.09573208540678024\n",
      "Epoch 90, Loss: 0.07465018332004547\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a9cce",
   "metadata": {},
   "source": [
    "Using sigmoid instead of relu would limit the weight outputs to between 0 and 1. While for some cases this would be beneficial, any sort of prediction that requires values beyond that range would be largely hindered. We can see that here our loss actually ends up higher than before which implies that our dataset of research papers is hindered by the limit of 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba833583",
   "metadata": {},
   "source": [
    "### 4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61c01e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.937464714050293\n",
      "Epoch 10, Loss: 0.7758140563964844\n",
      "Epoch 20, Loss: 0.2278340607881546\n",
      "Epoch 30, Loss: 0.07061011344194412\n",
      "Epoch 40, Loss: 0.028967803344130516\n",
      "Epoch 50, Loss: 0.015462013892829418\n",
      "Epoch 60, Loss: 0.010025356896221638\n",
      "Epoch 70, Loss: 0.007363757584244013\n",
      "Epoch 80, Loss: 0.005820433143526316\n",
      "Epoch 90, Loss: 0.004811818245798349\n",
      "Training complete!\n",
      "tensor(0.7256)\n"
     ]
    }
   ],
   "source": [
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_mask = torch.randperm(data.num_nodes) < int(0.1 * data.num_nodes)\n",
    "test_mask = ~train_mask\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    test_loss = criterion(out[test_mask], data.y[test_mask])\n",
    "\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e35f45",
   "metadata": {},
   "source": [
    "Using only 90% of the values for testing we would end up with a model that is very poor at generalizing. We can see this through the loss on the evaluated data being very high compared to the loss on the training dataset. This is due to overfitting as the model hasn't been exposed to a wide enough variety of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1213cb",
   "metadata": {},
   "source": [
    "### 5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708c478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9579170942306519\n",
      "Epoch 10, Loss: 0.03823234885931015\n",
      "Epoch 20, Loss: 0.013815916143357754\n",
      "Epoch 30, Loss: 0.00785114150494337\n",
      "Epoch 40, Loss: 0.005249897018074989\n",
      "Epoch 50, Loss: 0.0038193848449736834\n",
      "Epoch 60, Loss: 0.002943785861134529\n",
      "Epoch 70, Loss: 0.002356960903853178\n",
      "Epoch 80, Loss: 0.00193834921810776\n",
      "Epoch 90, Loss: 0.0016265342710539699\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c69a72f",
   "metadata": {},
   "source": [
    "RMS prop works well in scenarios where data has lots of variability and our research paper data has that which allows our data to converge much faster which we can see through our much lower learning and faster decrease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
